2006-12-15

-todo: working on model on board (red) for using directories and renaming
       for batch control...

- Each processor job has its inherent directory location which need not be externally specified.
  - within the directory is an input directory and an output directory, each divided into buckets.
    - input buckets are:
      - 1: receive bucket: receives rsync'd units (dir or file) transferred from outside
      - 2: work bucket: fully transferred units are renamed into this bucket by the sender
           - this bucket is monitored for work to accept by the job
      - 3: active bucket: holds work renamed from the input bucket by the job that is being processed by the job.
      - 4: processed bucket: holds work renamed from the active bucket by the job after fully processed.
    - output buckets are:
      - 1: processing bucket: holds output currently being generated by the job while processing active work.
      - 2: finished bucket: holds fully generated output renamed from the processing bucket by the job when completed.
           - this bucket is monitored by a forwarding job (JobForwarder)
      - 3: sent bucket: when data is to be forwarded to another job, fully transferred units are renamed to this bucket by the forwarding job.
           - the forwarding job renames next job's work bucket file/dir to receive bucket (ignoring error if file is missing), rsyncs to the next job's receive bucket, and renames completed units to the next job's work bucket.


- JobProcessor <>--- forwarder:JobForwarder <>--- next:JobProcessor
                 1 *                          1 *

Signals to JobProcessor (DirectoryMonitor)
- start forwarder (stopped by default)
- stop forwarder
  - stops monitoring for data to forward, but lets any in-progress rsyncs and renaming continue
x stop input monitor (started by default)
  x stops monitoring for data to process, but lets any in-progress processing continue
x start input monitor
x reset active [work unit]
  x stop input monitor if started,
  x kill any active processing threads,
    x keep a handle on the processor thread(s) and send them shutdown signals
  x delete output processing bucket contents;
  x rename input active bucket contents back to input work bucket,
  x restart input monitor if was started
x reset processed [work unit]
  - stop forwarder if started,
  x delete output finished and output sent data,
  - restart forwarder if was started;
  x stop input monitor if started,
  x rename input processed to input work,
  x restart input monitor if started
x reset all [work unit]
  x stop input monitor if started,
  x reset active [work unit],
  x reset processed [work unit],
  x start input monitor if was started
- purge  <-- ?just have this a manual pdsh command?
  - delete root job directory (all input and output directories)
- status
  - uptime, rate of processing, rate of forwarding, downtime waiting for work, downtime in "processing off"
  - whether processing (monitoring input)
  - whether forwarding
  - # of threads
  - # of running threads
  - # and names of active work units
  - # of work units being received
  - # of work units waiting for processing
  - # of processed work units
  - # of finished unsent work units
  - # of finished sent (forwarded) work units

When JobProcessor "comes up" it should automatically "reset active".


Job Model

- "JobProcessor"s are started on nodes and wait for signals

- on a "process" signal, the JobProcessor
  - checks whether the unit of work is already in progress or is completed
    - returns a signal indicating whether it's starting, finishing, or already did the requested work
  - reads its input and generates output (if necessary, or waits until the in-progress run is finished)
  - (a process signal optionally includes the signal and destination to send to next)
    - a destination ID is retrieved by the signal originator (through an "identify" signal) to
      - verify that the designated JobProcessor is up
      - retrieve the destination ID to use to identify the job processor on its node
  - the JobProcessor sends the next process signal (if present)
    - todo: define a mechanism for if/when this fails

- on a "process-all" signal, the JobProcessor
  - processes each unit of work present in the job's input area

- on a "re-process" signal, the JobProcessor
  - calls "purge" for the unit of work
  - calls "process" for the work unit

- on a "re-process-all" signal, the JobProcessor
  - re-processes each unit of work present in the job's input area

- on a "purge" signal, the JobProcessor
  - halts the unit of work if in progress
  - deletes output for unit of work if present

- on a "purge-all" signal, the JobProcessor
  - halts all work units in progress
  - deletes all output for all work units

- on an "identify" signal,
  - generate an ID to identify self for receiving signals/messages
    - JobManager routes to JobProcessor based on this "destination" ID

- on a "status" signal,
  - check on status of a specific unit of work
    - no-input, have-input-but-not-started, in-progress, completed
  - check on general status
    - number of received units of work
    - number of running units of work
    - number of completed units of work
    - time stats
      - uptime
      - min, max, average time per unit of work
  - number of "paused" (and/or "halted") processes

- "pause" or "pause-all" signal
  - pause current (or specified) running work units

- "resume" or "resume-all" signal
  - resume paused (or specified) or dropped/halted work units

- "halt", "halt-all" signal
  - safely abort current (or specified) work units for later resume

- DomainJobProcessor
  - single cumulative output file for all pages of domain
  - keep track of which pages we've begun/finished processing
  - dump all data for a page at once
  - jvm can die (or be killed) and job can be restarted and work can resume


- Need a controller/broker to
  - start job processors
  - send signals/messages to job processors
  - divvy out the work units to job processors
